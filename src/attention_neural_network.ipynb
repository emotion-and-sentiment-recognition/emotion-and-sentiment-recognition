{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680279ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (776, 10, 768)\n",
      "y shape: (776, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def transform_sequences(df: pd.DataFrame, max_sequence_length: int = 10) -> tuple:\n",
    "    \"\"\"Transform data into padded sequences for attention layer.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        max_sequence_length: Maximum length of sequence (will pad/truncate to this length)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_sequences, y_labels) where:\n",
    "            - X_sequences is a list of lists of embedded texts\n",
    "            - y_labels is corresponding labels for each sequence\n",
    "    \"\"\"\n",
    "    sequences = []  # Will store lists of texts\n",
    "    labels = []     # Will store corresponding labels\n",
    "    \n",
    "    current_sequence = []\n",
    "    \n",
    "    for index in df.index.tolist():\n",
    "        text = str(df.loc[index, 'text'])\n",
    "        \n",
    "        if text.startswith('#'):\n",
    "            if current_sequence:\n",
    "                # Add the sequence and its labels\n",
    "                sequences.append(current_sequence)\n",
    "                labels.append(df.loc[index, ['Joy', 'Trust', 'Anticipation', 'Surprise', \n",
    "                                           'Fear', 'Sadness', 'Disgust', 'Anger',\n",
    "                                           'Positive', 'Negative', 'Neutral']].values)\n",
    "                current_sequence = []\n",
    "        else:\n",
    "            current_sequence.append(text)\n",
    "    \n",
    "    # Handle any remaining sequence\n",
    "    if current_sequence:\n",
    "        sequences.append(current_sequence)\n",
    "        # Use neutral labels for sequences without hash\n",
    "        labels.append(np.zeros(11))\n",
    "    \n",
    "    # Convert labels to numpy array\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Pad sequences to max_sequence_length\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        # Truncate if longer than max_sequence_length\n",
    "        if len(seq) > max_sequence_length:\n",
    "            seq = seq[:max_sequence_length]\n",
    "        # Pad if shorter than max_sequence_length\n",
    "        elif len(seq) < max_sequence_length:\n",
    "            seq = seq + [''] * (max_sequence_length - len(seq))\n",
    "        padded_sequences.append(seq)\n",
    "    \n",
    "    # Embed each text in the sequences\n",
    "    model = SentenceTransformer('sdadas/st-polish-paraphrase-from-distilroberta')\n",
    "    embedded_sequences = []\n",
    "    \n",
    "    for sequence in padded_sequences:\n",
    "        # Embed each text in sequence\n",
    "        sequence_embeddings = []\n",
    "        for text in sequence:\n",
    "            if text:  # If not empty padding\n",
    "                embedding = model.encode(text)\n",
    "            else:  # For padding, use zero vector\n",
    "                embedding = np.zeros(768)  # 768 is the embedding dimension\n",
    "            sequence_embeddings.append(embedding)\n",
    "        embedded_sequences.append(sequence_embeddings)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    X_sequences = np.array(embedded_sequences)\n",
    "    y_labels = np.array(labels)\n",
    "    \n",
    "    return X_sequences, y_labels\n",
    "\n",
    "# Usage example:\n",
    "df = pd.read_csv('../../data/raw/train.csv')\n",
    "X_sequences, y_labels = transform_sequences(df)\n",
    "\n",
    "print(\"X shape:\", X_sequences.shape)  # Should be (num_sequences, max_sequence_length, embedding_dim)\n",
    "print(\"y shape:\", y_labels.shape)     # Should be (num_sequences, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f662a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_attention_model(max_sequence_length, embedding_dim, num_labels):\n",
    "    # Input shape: (batch_size, sequence_length, embedding_dim)\n",
    "    inputs = Input(shape=(max_sequence_length, embedding_dim))\n",
    "    \n",
    "    # Multi-head attention layer\n",
    "    attention = MultiHeadAttention(num_heads=8, key_dim=64)(inputs, inputs)\n",
    "    attention = Dropout(0.1)(attention)\n",
    "    attention = LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "    \n",
    "    # Dense layers processing each timestep independently\n",
    "    x = Dense(256, activation='relu')(attention)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Global average pooling to combine sequence\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(num_labels, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create and compile model\n",
    "model = create_attention_model(\n",
    "    max_sequence_length=10,\n",
    "    embedding_dim=768,\n",
    "    num_labels=11\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b4425a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sequences and labels for training\n",
    "train_df = pd.read_csv('../../data/raw/train.csv')\n",
    "val_df = pd.read_csv('../../data/raw/val.csv')\n",
    "test_df = pd.read_csv('../../data/raw/test.csv')\n",
    "\n",
    "# Transform data into sequences\n",
    "X_train, y_train = transform_sequences(train_df)\n",
    "X_val, y_val = transform_sequences(val_df)\n",
    "X_test, y_test = transform_sequences(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa576bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (776, 10, 768)\n",
      "y_train shape: (776, 11)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 11:08:20.383685: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 90ms/step - accuracy: 0.2181 - loss: 0.4636 - precision: 0.6402 - recall: 0.6496 - val_accuracy: 0.2874 - val_loss: 0.2807 - val_precision: 0.8488 - val_recall: 0.7768 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.3223 - loss: 0.2794 - precision: 0.8366 - recall: 0.7862 - val_accuracy: 0.2395 - val_loss: 0.3058 - val_precision: 0.8665 - val_recall: 0.7526 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.1868 - loss: 0.2751 - precision: 0.8482 - recall: 0.7781 - val_accuracy: 0.4012 - val_loss: 0.2990 - val_precision: 0.8387 - val_recall: 0.8183 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3878 - loss: 0.2430 - precision: 0.8730 - recall: 0.8258\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.3870 - loss: 0.2431 - precision: 0.8727 - recall: 0.8259 - val_accuracy: 0.3533 - val_loss: 0.3244 - val_precision: 0.8252 - val_recall: 0.7595 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.3758 - loss: 0.2349 - precision: 0.8708 - recall: 0.8344 - val_accuracy: 0.0958 - val_loss: 0.2889 - val_precision: 0.8455 - val_recall: 0.8045 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.3479 - loss: 0.2224 - precision: 0.8717 - recall: 0.8584 - val_accuracy: 0.1078 - val_loss: 0.2941 - val_precision: 0.8484 - val_recall: 0.8131 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import Precision, Recall\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_val = y_val.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Create and compile the attention model\n",
    "model = create_attention_model(\n",
    "    max_sequence_length=10,\n",
    "    embedding_dim=768,\n",
    "    num_labels=11\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    ")\n",
    "\n",
    "# Add callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Final score: 0.3731994532702938\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "texts_metrics = evaluate_texts(model, X_test=X_test, y_test=y_test)\n",
    "sentences_metrics = evaluate_sentences_v2(model, X_test=X_test, y_test=y_test)\n",
    "final_score = calculate_final_score(texts_metrics, sentences_metrics)\n",
    "\n",
    "print(\"Final score:\", final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d0687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d5ee194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "def evaluate_texts(model, X_test: pd.DataFrame, y_test: pd.DataFrame, threshold: float=0.5) -> Dict:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    # y_test = y_test.values\n",
    "    \n",
    "    labels = ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear', 'Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['F1-score macro texts'] = 0\n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = 0\n",
    "        metrics[f\"Recall {label}\"] = 0\n",
    "        metrics[f\"F1-score {label}\"] = 0\n",
    "        metrics[f\"TP {label}\"] = 0\n",
    "        metrics[f\"FP {label}\"] = 0\n",
    "        metrics[f\"TN {label}\"] = 0\n",
    "        metrics[f\"FN {label}\"] = 0\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"TP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 1)))\n",
    "        metrics[f\"FP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"TN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"FN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 1)))\n",
    "        \n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FP {label}\"] + 1e-8)\n",
    "        metrics[f\"Recall {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FN {label}\"] + 1e-8)\n",
    "        metrics[f\"F1-score {label}\"] = 2 * (metrics[f\"Precision {label}\"] * metrics[f\"Recall {label}\"]) / (metrics[f\"Precision {label}\"] + metrics[f\"Recall {label}\"] + 1e-8)\n",
    "\n",
    "    metrics['F1-score macro texts'] = sum(metrics[f\"F1-score {label}\"] for label in labels) / len(labels)\n",
    "    \n",
    "    metrics = {k: v for (k, v) in metrics.items() if ('Precision' in k) or ('Recall' in k) or ('F1' in k)}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d785fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentences_v1(model, X_test: pd.DataFrame, y_test: pd.DataFrame, threshold: float=0.5) -> Dict:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    # y_test = y_test.values\n",
    "    \n",
    "    labels = ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear', 'Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['F1-score macro sentences'] = 0\n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = 0\n",
    "        metrics[f\"Recall {label}\"] = 0\n",
    "        metrics[f\"F1-score {label}\"] = 0\n",
    "        metrics[f\"TP {label}\"] = 0\n",
    "        metrics[f\"FP {label}\"] = 0\n",
    "        metrics[f\"TN {label}\"] = 0\n",
    "        metrics[f\"FN {label}\"] = 0\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"TP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 1)))\n",
    "        metrics[f\"FP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"TN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"FN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 1)))\n",
    "        \n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FP {label}\"] + 1e-8)\n",
    "        metrics[f\"Recall {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FN {label}\"] + 1e-8)\n",
    "        metrics[f\"F1-score {label}\"] = 2 * (metrics[f\"Precision {label}\"] * metrics[f\"Recall {label}\"]) / (metrics[f\"Precision {label}\"] + metrics[f\"Recall {label}\"] + 1e-8)\n",
    "\n",
    "    metrics['F1-score macro sentences'] = sum(metrics[f\"F1-score {label}\"] for label in labels) / len(labels)\n",
    "\n",
    "    metrics = {k: v for (k, v) in metrics.items() if ('Precision' in k) or ('Recall' in k) or ('F1' in k)}\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30adb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def get_hash_indeces(df: pd.DataFrame) -> List:\n",
    "    hash_indices = []\n",
    "    for index in df.index.tolist():\n",
    "        if str(df.loc[index, 'text']).startswith('#'):\n",
    "            hash_indices.append(index)\n",
    "          \n",
    "    result_hash_indices = []   \n",
    "    i = 0;\n",
    "    for index in hash_indices:\n",
    "        index = index - i\n",
    "        result_hash_indices.append(index)\n",
    "        i += 1\n",
    "    \n",
    "    return result_hash_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e440e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentences_v2(model: keras.Model, X_test: pd.DataFrame, y_test: pd.DataFrame, threshold: float=1): # -> Dict:\n",
    "    raw_data = encode_labels(pd.read_csv('../../data/raw/test.csv'))\n",
    "    hash_indices = get_hash_indeces(raw_data)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    # y_test = y_test.values\n",
    "    \n",
    "    labels = ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear', 'Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['F1-score macro sentences'] = 0\n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = 0\n",
    "        metrics[f\"Recall {label}\"] = 0\n",
    "        metrics[f\"F1-score {label}\"] = 0\n",
    "        metrics[f\"TP {label}\"] = 0\n",
    "        metrics[f\"FP {label}\"] = 0\n",
    "        metrics[f\"TN {label}\"] = 0\n",
    "        metrics[f\"FN {label}\"] = 0\n",
    "    \n",
    "    y_true_segments = []\n",
    "    y_pred_segments = []\n",
    "    \n",
    "    i = 0\n",
    "    start = 0\n",
    "    for index in hash_indices:\n",
    "        pred_sum = np.zeros(11, dtype=np.float64)\n",
    "        for y_pred_i in y_pred[start:index]:\n",
    "            pred_sum += y_pred_i\n",
    "            \n",
    "        y_true_segments.append(raw_data.iloc[index+0, 1:].to_numpy())\n",
    "        y_pred_segments.append((pred_sum >= threshold).astype(int))\n",
    "        \n",
    "        # y_true_segments.append(raw_data.iloc[index+0, 1:].to_numpy())\n",
    "        # y_pred_segments.append((sum >= threshold).astype(int))\n",
    "            \n",
    "        i += 1\n",
    "        start = index\n",
    "\n",
    "    y_true_segments = np.array(y_true_segments)\n",
    "    y_pred_segments = np.array(y_pred_segments)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"TP {label}\"] = int(np.sum((y_pred_segments[:, i] == 1) & (y_true_segments[:, i] == 1)))\n",
    "        metrics[f\"FP {label}\"] = int(np.sum((y_pred_segments[:, i] == 1) & (y_true_segments[:, i] == 0)))\n",
    "        metrics[f\"TN {label}\"] = int(np.sum((y_pred_segments[:, i] == 0) & (y_true_segments[:, i] == 0)))\n",
    "        metrics[f\"FN {label}\"] = int(np.sum((y_pred_segments[:, i] == 0) & (y_true_segments[:, i] == 1)))\n",
    "        \n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FP {label}\"] + 1e-8)\n",
    "        metrics[f\"Recall {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FN {label}\"] + 1e-8)\n",
    "        metrics[f\"F1-score {label}\"] = 2 * (metrics[f\"Precision {label}\"] * metrics[f\"Recall {label}\"]) / (metrics[f\"Precision {label}\"] + metrics[f\"Recall {label}\"] + 1e-8)\n",
    "\n",
    "    metrics['F1-score macro sentences'] = sum(metrics[f\"F1-score {label}\"] for label in labels) / len(labels)\n",
    "\n",
    "    metrics = {k: v for (k, v) in metrics.items() if ('Precision' in k) or ('Recall' in k) or ('F1' in k)}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa686b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_score(text_metrics: Dict, sentences_metrics: Dict) -> float:\n",
    "    return (text_metrics['F1-score macro texts'] + sentences_metrics['F1-score macro sentences']) / 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
