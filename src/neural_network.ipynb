{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd6b618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plwordnet\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "wn = plwordnet.load('../data/plwordnet_4_2.xml')\n",
    "\n",
    "def generalize_text(text: str) -> str:\n",
    "    lemmatized_text = []\n",
    "\n",
    "    doc = nlp(text=text)\n",
    "    for token in doc:\n",
    "        lemmatized_text.append(token.lemma_.lower())\n",
    "\n",
    "    generalized_text = []\n",
    "    for word in lemmatized_text:\n",
    "\n",
    "        try:\n",
    "            if len(word <= 3):\n",
    "                generalized_text.append(word)\n",
    "            else:\n",
    "                lu = wn.find(word+'.2')\n",
    "                if len(wn.hypernyms(lu.synset)) < 1:\n",
    "                    generalized_text.append(word)\n",
    "                else:\n",
    "                    generalized_text.append(str(wn.hypernyms(lu.synset)[0])[1:-1])\n",
    "        except Exception as e:\n",
    "            generalized_text.append(word)\n",
    "\n",
    "    return ' '.join(generalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591324bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mchoj\\Documents\\GitHub\\emotion-and-sentiment-recognition\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def delete_hashs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[~df['text'].astype(str).str.startswith('#')]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def encode_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear','Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']:\n",
    "        df[col] = df[col].apply(lambda x: 1 if x else 0)\n",
    "    return df\n",
    "\n",
    "def generalize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for index in df.index.to_list():\n",
    "        df.loc[index, 'text'] = generalize_text(str(df.loc[index, 'text']))\n",
    "    return df\n",
    "\n",
    "def embed_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    model = SentenceTransformer('sdadas/st-polish-paraphrase-from-distilroberta')\n",
    "    \n",
    "    corpus = [str(df.loc[index, 'text']) for index in df.index.to_list()]\n",
    "    embeddings = model.encode(corpus)\n",
    "\n",
    "    embedding_column_names = [f'embedding_{i}' for i in range(embeddings.shape[1])]\n",
    "    \n",
    "    df_embeddings = pd.DataFrame(embeddings, columns=embedding_column_names)\n",
    "    df = pd.concat([df_embeddings, df], axis=1)\n",
    "    return df\n",
    "\n",
    "def rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = df.columns.str.lower()\n",
    "    return df\n",
    "\n",
    "def transform_texts(df: pd.DataFrame) -> pd.DataFrame:    \n",
    "    result_df = delete_hashs(df=df)\n",
    "    result_df = encode_labels(df=result_df)\n",
    "    result_df = generalize(df=result_df)\n",
    "    result_df = embed_text(df=result_df)\n",
    "    result_df = rename_columns(df=result_df)\n",
    "    return result_df\n",
    "\n",
    "def delete_empty(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df['text'].astype(str).str.len() > 2]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def transform_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_df = pd.DataFrame(data={\n",
    "        'text': [],\n",
    "        'Joy': [], 'Trust': [], 'Anticipation': [], 'Surprise': [], 'Fear': [], 'Sadness': [],\n",
    "           'Disgust': [], 'Anger': [], 'Positive': [], 'Negative': [], 'Neutral': []\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for index in df.index.tolist():\n",
    "        if (str)(df.loc[index, 'text']).startswith('#'):\n",
    "            sentence = \" \".join(sentences)\n",
    "            df.loc[index, 'text'] = sentence\n",
    "            result_df = pd.concat([result_df, df.loc[[index]]])\n",
    "            sentences = []\n",
    "        else:\n",
    "            sentences.append((str)(df.loc[index, 'text']))\n",
    "            \n",
    "    result_df = delete_hashs(df=result_df)\n",
    "    result_df = delete_empty(df=result_df)\n",
    "    result_df = encode_labels(df=result_df)\n",
    "    result_df = generalize(df=result_df)\n",
    "    result_df = embed_text(df=result_df)\n",
    "    result_df = rename_columns(df=result_df)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a11bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "def load_data() -> List:\n",
    "    data = []\n",
    "    \n",
    "    for name in ['train', 'val', 'test']:\n",
    "        for category in ['texts', 'sentences']:\n",
    "            if os.path.exists(f'../data/clean/nn_{name}_{category}.csv'):\n",
    "                df = pd.read_csv(f'../data/clean/nn_{name}_{category}.csv', index_col=0)\n",
    "            else:\n",
    "                df = pd.read_csv(f'../data/raw/{name}.csv')\n",
    "                if category == 'texts':\n",
    "                    df = transform_texts(df=df)\n",
    "                elif category == 'sentences':\n",
    "                    df = transform_sentences(df=df)\n",
    "                df.to_csv(f'../data/clean/nn_{name}_{category}.csv')\n",
    "\n",
    "            data.append(df)        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1339f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "train_texts = data[0]\n",
    "train_sentences = data[1]\n",
    "val_texts = data[2]\n",
    "val_sentences = data[3]\n",
    "test_texts = data[4]\n",
    "test_sentences = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec526ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>trust</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>surprise</th>\n",
       "      <th>fear</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>anger</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.562764</td>\n",
       "      <td>-0.397882</td>\n",
       "      <td>-0.267292</td>\n",
       "      <td>-0.567739</td>\n",
       "      <td>-1.149851</td>\n",
       "      <td>0.662654</td>\n",
       "      <td>0.128688</td>\n",
       "      <td>0.475320</td>\n",
       "      <td>-0.341749</td>\n",
       "      <td>0.943553</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.025572</td>\n",
       "      <td>-0.235864</td>\n",
       "      <td>-0.290113</td>\n",
       "      <td>-0.991783</td>\n",
       "      <td>-0.482610</td>\n",
       "      <td>-0.079471</td>\n",
       "      <td>-0.227736</td>\n",
       "      <td>-1.217292</td>\n",
       "      <td>0.438888</td>\n",
       "      <td>-0.221104</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.475408</td>\n",
       "      <td>0.156776</td>\n",
       "      <td>-0.210054</td>\n",
       "      <td>-1.350016</td>\n",
       "      <td>-0.126225</td>\n",
       "      <td>-0.365511</td>\n",
       "      <td>0.108856</td>\n",
       "      <td>-0.143321</td>\n",
       "      <td>0.438478</td>\n",
       "      <td>0.077761</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.336756</td>\n",
       "      <td>0.084884</td>\n",
       "      <td>-0.059477</td>\n",
       "      <td>-0.480968</td>\n",
       "      <td>0.186055</td>\n",
       "      <td>-1.020058</td>\n",
       "      <td>-0.095881</td>\n",
       "      <td>0.175767</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.103957</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.477066</td>\n",
       "      <td>-0.384985</td>\n",
       "      <td>0.092087</td>\n",
       "      <td>-0.471120</td>\n",
       "      <td>-0.756108</td>\n",
       "      <td>-0.020734</td>\n",
       "      <td>-0.597373</td>\n",
       "      <td>-0.533785</td>\n",
       "      <td>0.817650</td>\n",
       "      <td>-0.484433</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.496382</td>\n",
       "      <td>-0.243892</td>\n",
       "      <td>-0.093434</td>\n",
       "      <td>-0.223938</td>\n",
       "      <td>0.163552</td>\n",
       "      <td>-0.107771</td>\n",
       "      <td>-0.155038</td>\n",
       "      <td>-0.587985</td>\n",
       "      <td>-0.201445</td>\n",
       "      <td>0.310824</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.118517</td>\n",
       "      <td>0.140505</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.213710</td>\n",
       "      <td>-0.135078</td>\n",
       "      <td>-0.422372</td>\n",
       "      <td>0.039289</td>\n",
       "      <td>-0.559245</td>\n",
       "      <td>-0.158402</td>\n",
       "      <td>-0.107544</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.097196</td>\n",
       "      <td>-0.078515</td>\n",
       "      <td>-0.249001</td>\n",
       "      <td>-0.379476</td>\n",
       "      <td>-0.063541</td>\n",
       "      <td>-0.161061</td>\n",
       "      <td>-0.200802</td>\n",
       "      <td>-0.974016</td>\n",
       "      <td>-0.269598</td>\n",
       "      <td>-0.232394</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.251442</td>\n",
       "      <td>0.039844</td>\n",
       "      <td>-0.041393</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.021001</td>\n",
       "      <td>-0.140833</td>\n",
       "      <td>0.306735</td>\n",
       "      <td>0.057826</td>\n",
       "      <td>-0.162801</td>\n",
       "      <td>-0.397339</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.270282</td>\n",
       "      <td>0.079601</td>\n",
       "      <td>0.281395</td>\n",
       "      <td>-0.319867</td>\n",
       "      <td>0.225556</td>\n",
       "      <td>-0.520233</td>\n",
       "      <td>-0.212180</td>\n",
       "      <td>-0.398634</td>\n",
       "      <td>0.146555</td>\n",
       "      <td>-0.211571</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 780 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "0     0.562764    -0.397882    -0.267292    -0.567739    -1.149851   \n",
       "1    -0.025572    -0.235864    -0.290113    -0.991783    -0.482610   \n",
       "2    -0.475408     0.156776    -0.210054    -1.350016    -0.126225   \n",
       "3    -0.336756     0.084884    -0.059477    -0.480968     0.186055   \n",
       "4    -0.477066    -0.384985     0.092087    -0.471120    -0.756108   \n",
       "5    -0.496382    -0.243892    -0.093434    -0.223938     0.163552   \n",
       "6     0.118517     0.140505    -0.107383    -0.213710    -0.135078   \n",
       "7     0.097196    -0.078515    -0.249001    -0.379476    -0.063541   \n",
       "8     0.251442     0.039844    -0.041393     0.003911     0.021001   \n",
       "9     0.270282     0.079601     0.281395    -0.319867     0.225556   \n",
       "\n",
       "   embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "0     0.662654     0.128688     0.475320    -0.341749     0.943553  ...   \n",
       "1    -0.079471    -0.227736    -1.217292     0.438888    -0.221104  ...   \n",
       "2    -0.365511     0.108856    -0.143321     0.438478     0.077761  ...   \n",
       "3    -1.020058    -0.095881     0.175767    -0.000036     0.103957  ...   \n",
       "4    -0.020734    -0.597373    -0.533785     0.817650    -0.484433  ...   \n",
       "5    -0.107771    -0.155038    -0.587985    -0.201445     0.310824  ...   \n",
       "6    -0.422372     0.039289    -0.559245    -0.158402    -0.107544  ...   \n",
       "7    -0.161061    -0.200802    -0.974016    -0.269598    -0.232394  ...   \n",
       "8    -0.140833     0.306735     0.057826    -0.162801    -0.397339  ...   \n",
       "9    -0.520233    -0.212180    -0.398634     0.146555    -0.211571  ...   \n",
       "\n",
       "   trust  anticipation  surprise  fear  sadness  disgust  anger  positive  \\\n",
       "0      0             0         1     0        1        0      1         0   \n",
       "1      0             0         0     0        1        1      1         0   \n",
       "2      0             0         0     0        0        1      1         0   \n",
       "3      0             0         0     0        1        1      1         0   \n",
       "4      0             0         0     0        0        0      1         0   \n",
       "5      0             0         0     0        0        0      0         1   \n",
       "6      0             0         0     0        0        0      0         1   \n",
       "7      0             0         1     0        1        0      0         0   \n",
       "8      0             0         0     0        0        0      1         1   \n",
       "9      0             0         0     0        0        0      1         0   \n",
       "\n",
       "   negative  neutral  \n",
       "0         1        0  \n",
       "1         1        0  \n",
       "2         1        0  \n",
       "3         1        0  \n",
       "4         1        1  \n",
       "5         0        1  \n",
       "6         0        1  \n",
       "7         1        0  \n",
       "8         1        1  \n",
       "9         1        0  \n",
       "\n",
       "[10 rows x 780 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc06e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>trust</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>surprise</th>\n",
       "      <th>fear</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>anger</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.107992</td>\n",
       "      <td>-0.544149</td>\n",
       "      <td>0.053766</td>\n",
       "      <td>-0.632760</td>\n",
       "      <td>-0.311535</td>\n",
       "      <td>-0.581503</td>\n",
       "      <td>-0.313330</td>\n",
       "      <td>-0.348168</td>\n",
       "      <td>0.349289</td>\n",
       "      <td>-0.406877</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.311132</td>\n",
       "      <td>-0.028671</td>\n",
       "      <td>0.441330</td>\n",
       "      <td>0.269969</td>\n",
       "      <td>-0.503570</td>\n",
       "      <td>-0.387672</td>\n",
       "      <td>0.084179</td>\n",
       "      <td>0.129839</td>\n",
       "      <td>0.064349</td>\n",
       "      <td>-0.002668</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059933</td>\n",
       "      <td>-0.061721</td>\n",
       "      <td>0.278209</td>\n",
       "      <td>-0.076590</td>\n",
       "      <td>-0.328901</td>\n",
       "      <td>-0.568747</td>\n",
       "      <td>0.589950</td>\n",
       "      <td>-0.501475</td>\n",
       "      <td>0.192955</td>\n",
       "      <td>-0.235708</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.768429</td>\n",
       "      <td>0.329751</td>\n",
       "      <td>-0.046399</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>-0.539526</td>\n",
       "      <td>-0.464502</td>\n",
       "      <td>0.219784</td>\n",
       "      <td>-0.005870</td>\n",
       "      <td>0.543218</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.079323</td>\n",
       "      <td>-0.283436</td>\n",
       "      <td>-0.049123</td>\n",
       "      <td>0.551994</td>\n",
       "      <td>-0.150095</td>\n",
       "      <td>-0.236103</td>\n",
       "      <td>-0.445485</td>\n",
       "      <td>0.063526</td>\n",
       "      <td>-0.037673</td>\n",
       "      <td>0.159559</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008165</td>\n",
       "      <td>-0.044767</td>\n",
       "      <td>-0.019482</td>\n",
       "      <td>-0.543493</td>\n",
       "      <td>0.151010</td>\n",
       "      <td>-0.438446</td>\n",
       "      <td>-0.115624</td>\n",
       "      <td>-0.558962</td>\n",
       "      <td>-0.040636</td>\n",
       "      <td>-0.198771</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.192084</td>\n",
       "      <td>0.341302</td>\n",
       "      <td>0.191277</td>\n",
       "      <td>-0.035575</td>\n",
       "      <td>-0.619834</td>\n",
       "      <td>-0.517675</td>\n",
       "      <td>0.101253</td>\n",
       "      <td>-0.099043</td>\n",
       "      <td>-0.411317</td>\n",
       "      <td>-0.308393</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.034829</td>\n",
       "      <td>-0.051287</td>\n",
       "      <td>0.388698</td>\n",
       "      <td>0.251668</td>\n",
       "      <td>-0.303737</td>\n",
       "      <td>-0.325470</td>\n",
       "      <td>-0.421632</td>\n",
       "      <td>0.267487</td>\n",
       "      <td>-0.006021</td>\n",
       "      <td>-0.086745</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.085704</td>\n",
       "      <td>0.031369</td>\n",
       "      <td>0.434524</td>\n",
       "      <td>-0.142889</td>\n",
       "      <td>-0.652816</td>\n",
       "      <td>-0.535798</td>\n",
       "      <td>0.209822</td>\n",
       "      <td>-0.081895</td>\n",
       "      <td>-0.156835</td>\n",
       "      <td>-0.037204</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.134724</td>\n",
       "      <td>0.155525</td>\n",
       "      <td>0.143574</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>-0.135043</td>\n",
       "      <td>-0.729203</td>\n",
       "      <td>-0.419711</td>\n",
       "      <td>-0.199815</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.060348</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 780 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "0    -0.107992    -0.544149     0.053766    -0.632760    -0.311535   \n",
       "1     0.311132    -0.028671     0.441330     0.269969    -0.503570   \n",
       "2    -0.059933    -0.061721     0.278209    -0.076590    -0.328901   \n",
       "3    -0.768429     0.329751    -0.046399     0.045369    -0.539526   \n",
       "4    -0.079323    -0.283436    -0.049123     0.551994    -0.150095   \n",
       "5     0.008165    -0.044767    -0.019482    -0.543493     0.151010   \n",
       "6     0.192084     0.341302     0.191277    -0.035575    -0.619834   \n",
       "7    -0.034829    -0.051287     0.388698     0.251668    -0.303737   \n",
       "8    -0.085704     0.031369     0.434524    -0.142889    -0.652816   \n",
       "9     0.134724     0.155525     0.143574     0.011912    -0.135043   \n",
       "\n",
       "   embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "0    -0.581503    -0.313330    -0.348168     0.349289    -0.406877  ...   \n",
       "1    -0.387672     0.084179     0.129839     0.064349    -0.002668  ...   \n",
       "2    -0.568747     0.589950    -0.501475     0.192955    -0.235708  ...   \n",
       "3    -0.464502     0.219784    -0.005870     0.543218     0.019147  ...   \n",
       "4    -0.236103    -0.445485     0.063526    -0.037673     0.159559  ...   \n",
       "5    -0.438446    -0.115624    -0.558962    -0.040636    -0.198771  ...   \n",
       "6    -0.517675     0.101253    -0.099043    -0.411317    -0.308393  ...   \n",
       "7    -0.325470    -0.421632     0.267487    -0.006021    -0.086745  ...   \n",
       "8    -0.535798     0.209822    -0.081895    -0.156835    -0.037204  ...   \n",
       "9    -0.729203    -0.419711    -0.199815    -0.166667     0.060348  ...   \n",
       "\n",
       "   trust  anticipation  surprise  fear  sadness  disgust  anger  positive  \\\n",
       "0      0             0         1     0        1        1      1         0   \n",
       "1      0             0         0     0        0        0      0         0   \n",
       "2      0             0         0     0        0        0      0         1   \n",
       "3      0             0         0     0        1        0      0         1   \n",
       "4      1             0         0     0        0        0      0         1   \n",
       "5      0             0         0     0        1        0      1         0   \n",
       "6      0             1         0     0        0        0      0         1   \n",
       "7      0             0         0     0        1        0      0         0   \n",
       "8      1             1         0     0        0        0      0         1   \n",
       "9      0             0         0     0        0        0      0         1   \n",
       "\n",
       "   negative  neutral  \n",
       "0         1        0  \n",
       "1         0        1  \n",
       "2         0        1  \n",
       "3         1        0  \n",
       "4         0        0  \n",
       "5         1        0  \n",
       "6         0        0  \n",
       "7         1        0  \n",
       "8         0        1  \n",
       "9         0        0  \n",
       "\n",
       "[10 rows x 780 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06400c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts = train_texts.iloc[:, :768] # + train_sentences.iloc[:, :768]\n",
    "y_train_texts = train_texts.iloc[:, 769:] # + train_sentences.iloc[:, 769:]\n",
    "\n",
    "X_val_texts = val_texts.iloc[:, :768] # + val_sentences.iloc[:, :768]\n",
    "y_val_texts = val_texts.iloc[:, 769:] # + val_sentences.iloc[:, 769:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a7cb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mchoj\\Documents\\GitHub\\emotion-and-sentiment-recognition\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "input_dim = X_train_texts.shape[1]\n",
    "output_dim = y_train_texts.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1024, activation='relu', input_shape=(input_dim, )),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(output_dim, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db176f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mchoj\\Documents\\GitHub\\emotion-and-sentiment-recognition\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=[\n",
    "        'binary_accuracy',\n",
    "        Precision(thresholds=0.5, name='precision'),\n",
    "        Recall(thresholds=0.5, name='recall'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d390976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">787,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m787,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │         \u001b[38;5;34m1,419\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,830,539</span> (6.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,830,539\u001b[0m (6.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,826,699</span> (6.97 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,826,699\u001b[0m (6.97 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> (15.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,840\u001b[0m (15.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf3598f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - binary_accuracy: 0.7125 - loss: 0.5749 - precision: 0.4917 - recall: 0.6042 - val_binary_accuracy: 0.8488 - val_loss: 0.3521 - val_precision: 0.7836 - val_recall: 0.6341 - learning_rate: 0.0010\n",
      "Epoch 2/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - binary_accuracy: 0.8316 - loss: 0.3866 - precision: 0.7310 - recall: 0.5951 - val_binary_accuracy: 0.8527 - val_loss: 0.3444 - val_precision: 0.7917 - val_recall: 0.6414 - learning_rate: 0.0010\n",
      "Epoch 3/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - binary_accuracy: 0.8475 - loss: 0.3618 - precision: 0.7630 - recall: 0.6247 - val_binary_accuracy: 0.8603 - val_loss: 0.3321 - val_precision: 0.7930 - val_recall: 0.6768 - learning_rate: 0.0010\n",
      "Epoch 4/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - binary_accuracy: 0.8527 - loss: 0.3535 - precision: 0.7749 - recall: 0.6328 - val_binary_accuracy: 0.8628 - val_loss: 0.3312 - val_precision: 0.7983 - val_recall: 0.6813 - learning_rate: 0.0010\n",
      "Epoch 5/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - binary_accuracy: 0.8587 - loss: 0.3381 - precision: 0.7836 - recall: 0.6548 - val_binary_accuracy: 0.8605 - val_loss: 0.3347 - val_precision: 0.8150 - val_recall: 0.6480 - learning_rate: 0.0010\n",
      "Epoch 6/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - binary_accuracy: 0.8552 - loss: 0.3396 - precision: 0.7789 - recall: 0.6479 - val_binary_accuracy: 0.8617 - val_loss: 0.3292 - val_precision: 0.8041 - val_recall: 0.6678 - learning_rate: 0.0010\n",
      "Epoch 7/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - binary_accuracy: 0.8620 - loss: 0.3329 - precision: 0.7952 - recall: 0.6609 - val_binary_accuracy: 0.8591 - val_loss: 0.3300 - val_precision: 0.7932 - val_recall: 0.6704 - learning_rate: 0.0010\n",
      "Epoch 8/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - binary_accuracy: 0.8614 - loss: 0.3295 - precision: 0.7866 - recall: 0.6688 - val_binary_accuracy: 0.8575 - val_loss: 0.3359 - val_precision: 0.7842 - val_recall: 0.6762 - learning_rate: 0.0010\n",
      "Epoch 9/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - binary_accuracy: 0.8625 - loss: 0.3293 - precision: 0.7920 - recall: 0.6660 - val_binary_accuracy: 0.8614 - val_loss: 0.3265 - val_precision: 0.7973 - val_recall: 0.6760 - learning_rate: 0.0010\n",
      "Epoch 10/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - binary_accuracy: 0.8642 - loss: 0.3211 - precision: 0.7901 - recall: 0.6774 - val_binary_accuracy: 0.8604 - val_loss: 0.3288 - val_precision: 0.8018 - val_recall: 0.6646 - learning_rate: 0.0010\n",
      "Epoch 11/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - binary_accuracy: 0.8691 - loss: 0.3172 - precision: 0.8000 - recall: 0.6818 - val_binary_accuracy: 0.8642 - val_loss: 0.3240 - val_precision: 0.7974 - val_recall: 0.6892 - learning_rate: 0.0010\n",
      "Epoch 12/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - binary_accuracy: 0.8668 - loss: 0.3182 - precision: 0.7950 - recall: 0.6826 - val_binary_accuracy: 0.8594 - val_loss: 0.3270 - val_precision: 0.8111 - val_recall: 0.6475 - learning_rate: 0.0010\n",
      "Epoch 13/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - binary_accuracy: 0.8699 - loss: 0.3138 - precision: 0.8042 - recall: 0.6887 - val_binary_accuracy: 0.8636 - val_loss: 0.3251 - val_precision: 0.8078 - val_recall: 0.6715 - learning_rate: 0.0010\n",
      "Epoch 14/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - binary_accuracy: 0.8727 - loss: 0.3050 - precision: 0.8073 - recall: 0.6951 - val_binary_accuracy: 0.8651 - val_loss: 0.3254 - val_precision: 0.8132 - val_recall: 0.6715 - learning_rate: 0.0010\n",
      "Epoch 15/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - binary_accuracy: 0.8748 - loss: 0.3020 - precision: 0.8119 - recall: 0.6938 - val_binary_accuracy: 0.8635 - val_loss: 0.3373 - val_precision: 0.7959 - val_recall: 0.6878 - learning_rate: 0.0010\n",
      "Epoch 16/16\n",
      "\u001b[1m797/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - binary_accuracy: 0.8775 - loss: 0.3020 - precision: 0.8158 - recall: 0.7014\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - binary_accuracy: 0.8775 - loss: 0.3020 - precision: 0.8158 - recall: 0.7013 - val_binary_accuracy: 0.8593 - val_loss: 0.3385 - val_precision: 0.7943 - val_recall: 0.6699 - learning_rate: 0.0010\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_texts, y_train_texts,\n",
    "    epochs=16,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_val_texts, y_val_texts),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba875d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/neural_network.keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f149ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "history = keras.models.load_model('../models/neural_network.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d5ee194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "def evaluate_texts(model, X_test: pd.DataFrame, y_test: pd.DataFrame, threshold: float=0.5) -> Dict:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    y_test = y_test.values\n",
    "    \n",
    "    labels = ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear', 'Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['F1-score macro texts'] = 0\n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = 0\n",
    "        metrics[f\"Recall {label}\"] = 0\n",
    "        metrics[f\"F1-score {label}\"] = 0\n",
    "        metrics[f\"TP {label}\"] = 0\n",
    "        metrics[f\"FP {label}\"] = 0\n",
    "        metrics[f\"TN {label}\"] = 0\n",
    "        metrics[f\"FN {label}\"] = 0\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"TP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 1)))\n",
    "        metrics[f\"FP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"TN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"FN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 1)))\n",
    "        \n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FP {label}\"] + 1e-8)\n",
    "        metrics[f\"Recall {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FN {label}\"] + 1e-8)\n",
    "        metrics[f\"F1-score {label}\"] = 2 * (metrics[f\"Precision {label}\"] * metrics[f\"Recall {label}\"]) / (metrics[f\"Precision {label}\"] + metrics[f\"Recall {label}\"] + 1e-8)\n",
    "\n",
    "    metrics['F1-score macro texts'] = sum(metrics[f\"F1-score {label}\"] for label in labels) / len(labels)\n",
    "    \n",
    "    metrics = {k: v for (k, v) in metrics.items() if ('Precision' in k) or ('Recall' in k) or ('F1' in k)}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d785fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentences_v1(model, X_test: pd.DataFrame, y_test: pd.DataFrame, threshold: float=0.5) -> Dict:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    y_test = y_test.values\n",
    "    \n",
    "    labels = ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear', 'Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['F1-score macro sentences'] = 0\n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = 0\n",
    "        metrics[f\"Recall {label}\"] = 0\n",
    "        metrics[f\"F1-score {label}\"] = 0\n",
    "        metrics[f\"TP {label}\"] = 0\n",
    "        metrics[f\"FP {label}\"] = 0\n",
    "        metrics[f\"TN {label}\"] = 0\n",
    "        metrics[f\"FN {label}\"] = 0\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"TP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 1)))\n",
    "        metrics[f\"FP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"TN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"FN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 1)))\n",
    "        \n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FP {label}\"] + 1e-8)\n",
    "        metrics[f\"Recall {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FN {label}\"] + 1e-8)\n",
    "        metrics[f\"F1-score {label}\"] = 2 * (metrics[f\"Precision {label}\"] * metrics[f\"Recall {label}\"]) / (metrics[f\"Precision {label}\"] + metrics[f\"Recall {label}\"] + 1e-8)\n",
    "\n",
    "    metrics['F1-score macro sentences'] = sum(metrics[f\"F1-score {label}\"] for label in labels) / len(labels)\n",
    "\n",
    "    metrics = {k: v for (k, v) in metrics.items() if ('Precision' in k) or ('Recall' in k) or ('F1' in k)}\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30adb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_indeces(df: pd.DataFrame) -> List:\n",
    "    hash_indices = []\n",
    "    for index in df.index.tolist():\n",
    "        if str(df.loc[index, 'text']).startswith('#'):\n",
    "            hash_indices.append(index)\n",
    "          \n",
    "    result_hash_indices = []   \n",
    "    i = 0\n",
    "    for index in hash_indices:\n",
    "        index = index - i\n",
    "        result_hash_indices.append(index)\n",
    "        i += 1\n",
    "    \n",
    "    return result_hash_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e440e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentences_v2(model: keras.Model, X_test: pd.DataFrame, y_test: pd.DataFrame, threshold: float=1): # -> Dict:\n",
    "    raw_data = encode_labels(pd.read_csv('../data/raw/test.csv'))\n",
    "    hash_indices = get_hash_indeces(raw_data)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    labels = ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear', 'Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['F1-score macro sentences'] = 0\n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = 0\n",
    "        metrics[f\"Recall {label}\"] = 0\n",
    "        metrics[f\"F1-score {label}\"] = 0\n",
    "        metrics[f\"TP {label}\"] = 0\n",
    "        metrics[f\"FP {label}\"] = 0\n",
    "        metrics[f\"TN {label}\"] = 0\n",
    "        metrics[f\"FN {label}\"] = 0\n",
    "    \n",
    "    y_true_segments = []\n",
    "    y_pred_segments = []\n",
    "    \n",
    "    i = 0\n",
    "    start = 0\n",
    "    for index in hash_indices:\n",
    "        pred_sum = np.zeros(11, dtype=np.float64)\n",
    "        for y_pred_i in y_pred[start:index]:\n",
    "            pred_sum += y_pred_i\n",
    "            \n",
    "        y_true_segments.append(raw_data.iloc[index+0, 1:].to_numpy())\n",
    "        y_pred_segments.append((pred_sum >= threshold).astype(int))\n",
    "                    \n",
    "        i += 1\n",
    "        start = index\n",
    "\n",
    "    y_true_segments = np.array(y_true_segments)\n",
    "    y_pred_segments = np.array(y_pred_segments)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"TP {label}\"] = int(np.sum((y_pred_segments[:, i] == 1) & (y_true_segments[:, i] == 1)))\n",
    "        metrics[f\"FP {label}\"] = int(np.sum((y_pred_segments[:, i] == 1) & (y_true_segments[:, i] == 0)))\n",
    "        metrics[f\"TN {label}\"] = int(np.sum((y_pred_segments[:, i] == 0) & (y_true_segments[:, i] == 0)))\n",
    "        metrics[f\"FN {label}\"] = int(np.sum((y_pred_segments[:, i] == 0) & (y_true_segments[:, i] == 1)))\n",
    "        \n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FP {label}\"] + 1e-8)\n",
    "        metrics[f\"Recall {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FN {label}\"] + 1e-8)\n",
    "        metrics[f\"F1-score {label}\"] = 2 * (metrics[f\"Precision {label}\"] * metrics[f\"Recall {label}\"]) / (metrics[f\"Precision {label}\"] + metrics[f\"Recall {label}\"] + 1e-8)\n",
    "\n",
    "    metrics['F1-score macro sentences'] = sum(metrics[f\"F1-score {label}\"] for label in labels) / len(labels)\n",
    "\n",
    "    metrics = {k: v for (k, v) in metrics.items() if ('Precision' in k) or ('Recall' in k) or ('F1' in k)}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36c353e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "{'F1-score macro texts': 0.570798261912883, 'Precision Joy': 0.7992766726799407, 'Recall Joy': 0.8036363636217521, 'F1-score Joy': 0.8014505842874088, 'Precision Trust': 0.6301369862725965, 'Recall Trust': 0.6602870813081202, 'F1-score Trust': 0.6448598080567081, 'Precision Anticipation': 0.5303030302226814, 'Recall Anticipation': 0.2799999999776, 'F1-score Anticipation': 0.3664921420355803, 'Precision Surprise': 0.0, 'Recall Surprise': 0.0, 'F1-score Surprise': 0.0, 'Precision Fear': 0.6999999992999999, 'Recall Fear': 0.11864406777650101, 'F1-score Fear': 0.20289854818735562, 'Precision Sadness': 0.8259385665388065, 'Recall Sadness': 0.879999999984, 'F1-score Sadness': 0.8521126710463574, 'Precision Disgust': 0.6249999999660326, 'Recall Disgust': 0.473251028787109, 'F1-score Disgust': 0.5386416812529001, 'Precision Anger': 0.6739130434294266, 'Recall Anger': 0.44285714283605443, 'F1-score Anger': 0.5344827538040032, 'Precision Positive': 0.8242122719597975, 'Recall Positive': 0.7964743589615949, 'F1-score Positive': 0.8101059444585126, 'Precision Negative': 0.8295819935557945, 'Recall Negative': 0.8927335639983957, 'F1-score Negative': 0.859999994992389, 'Precision Neutral': 0.8927038626226307, 'Recall Neutral': 0.5333333333196582, 'F1-score Neutral': 0.6677367529204982}\n"
     ]
    }
   ],
   "source": [
    "texts_metrics = evaluate_texts(history, X_test=test_texts.iloc[:, :768], y_test=test_texts.iloc[:, 769:], threshold=0.5)\n",
    "print(texts_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a702b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "{'F1-score macro sentences': 0.621044168696023, 'Precision Joy': 0.8554216866439251, 'Recall Joy': 0.8255813952528395, 'F1-score Joy': 0.840236681292672, 'Precision Trust': 0.6363636361707988, 'Recall Trust': 0.6999999997666666, 'F1-score Trust': 0.6666666614663644, 'Precision Anticipation': 0.7142857132653061, 'Recall Anticipation': 0.41666666631944443, 'F1-score Anticipation': 0.5263157842659281, 'Precision Surprise': 0.0, 'Recall Surprise': 0.0, 'F1-score Surprise': 0.0, 'Precision Fear': 0.3333333322222222, 'Recall Fear': 0.08333333326388888, 'F1-score Fear': 0.1333333299555556, 'Precision Sadness': 0.8947368420110804, 'Recall Sadness': 0.9340659339632895, 'F1-score Sadness': 0.9139784895276911, 'Precision Disgust': 0.6486486484733381, 'Recall Disgust': 0.5333333332148148, 'F1-score Disgust': 0.5853658485633552, 'Precision Anger': 0.8787878785215794, 'Recall Anger': 0.6444444443012346, 'F1-score Anger': 0.7435897385174228, 'Precision Positive': 0.8526315788576178, 'Recall Positive': 0.8350515463056648, 'F1-score Positive': 0.843749994912652, 'Precision Negative': 0.8823529410899654, 'Recall Negative': 0.9677419353798128, 'F1-score Negative': 0.9230769179928995, 'Precision Neutral': 0.9999999994736841, 'Recall Neutral': 0.48717948705456937, 'F1-score Neutral': 0.6551724091617123}\n"
     ]
    }
   ],
   "source": [
    "sentences_metrics_v1 = evaluate_sentences_v1(history, X_test=test_sentences.iloc[:, :768], y_test=test_sentences.iloc[:, 769:], threshold=0.5)\n",
    "print(sentences_metrics_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78d16f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "{'F1-score macro sentences': 0.1677332432885955, 'Precision Joy': 0.4999999998076923, 'Recall Joy': 0.18309859152350727, 'F1-score Joy': 0.2680412331342332, 'Precision Trust': 0.20833333324652778, 'Recall Trust': 0.19999999992, 'F1-score Trust': 0.2040816275718452, 'Precision Anticipation': 0.049999999975, 'Recall Anticipation': 0.0624999999609375, 'F1-score Anticipation': 0.05555555058642019, 'Precision Surprise': 0.13333333324444444, 'Recall Surprise': 0.1999999998, 'F1-score Surprise': 0.15999999507200013, 'Precision Fear': 0.0, 'Recall Fear': 0.0, 'F1-score Fear': 0.0, 'Precision Sadness': 0.5172413791319858, 'Recall Sadness': 0.19230769228303748, 'F1-score Sadness': 0.2803738277718579, 'Precision Disgust': 0.13043478255198487, 'Recall Disgust': 0.09999999996666667, 'F1-score Disgust': 0.11320754221431137, 'Precision Anger': 0.13043478255198487, 'Recall Anger': 0.0909090908815427, 'F1-score Anger': 0.10714285226403084, 'Precision Positive': 0.5384615382544379, 'Recall Positive': 0.18181818179456907, 'F1-score Positive': 0.27184465636723537, 'Precision Negative': 0.5172413791319858, 'Recall Negative': 0.18749999997656253, 'F1-score Negative': 0.2752293538422692, 'Precision Neutral': 0.2222222220987654, 'Recall Neutral': 0.07272727271404958, 'F1-score Neutral': 0.10958903735034728}\n"
     ]
    }
   ],
   "source": [
    "sentences_metrics_v2 = evaluate_sentences_v2(history, X_test=test_sentences.iloc[:, :768], y_test=test_sentences.iloc[:, 769:], threshold=0.5)\n",
    "print(sentences_metrics_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa686b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_score(text_metrics: Dict, sentences_metrics: Dict) -> float:\n",
    "    return (text_metrics['F1-score macro texts'] + sentences_metrics['F1-score macro sentences']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "677e8105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.595921215304453"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_final_score(texts_metrics, sentences_metrics_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c70a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36926575260073924"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_final_score(texts_metrics, sentences_metrics_v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
