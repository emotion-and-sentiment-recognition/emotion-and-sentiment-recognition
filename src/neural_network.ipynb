{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5183f7a5",
   "metadata": {},
   "source": [
    "# Lemmantization + WordNet + BERT + Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928800d",
   "metadata": {},
   "source": [
    "Uzywamy pakoetu plwordnet oraz spacy, aby opracować dane w języku polskim. Najpierw lematyzujemy tekst, czyli zamieniamy każde słowo na jego podstawową formę. Potem próbujemy każde z tych słów zgeneralizować, zatem sprawdzamy, czy w słowniku plWordNet istnieje jakieś bardziej ogólne słowo, do którego to się odnosi. Jeśli istnieje – to je podmieniamy. Jeśli nie, albo coś nie działa, to zostawiamy tak jak było. Dzięki temu tekst staje się bardziej ogólny, co może pomóc modelowi lepiej rozumieć sens zdania niezależnie od konkretnych słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd6b618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plwordnet\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pl_core_news_sm')\n",
    "wn = plwordnet.load('../data/plwordnet_4_2.xml')\n",
    "\n",
    "def generalize_text(text: str) -> str:\n",
    "    lemmatized_text = []\n",
    "\n",
    "    doc = nlp(text=text)\n",
    "    for token in doc:\n",
    "        lemmatized_text.append(token.lemma_.lower())\n",
    "\n",
    "    generalized_text = []\n",
    "    for word in lemmatized_text:\n",
    "\n",
    "        try:\n",
    "            if len(word <= 3):\n",
    "                generalized_text.append(word)\n",
    "            else:\n",
    "                lu = wn.find(word+'.2')\n",
    "                if len(wn.hypernyms(lu.synset)) < 1:\n",
    "                    generalized_text.append(word)\n",
    "                else:\n",
    "                    generalized_text.append(str(wn.hypernyms(lu.synset)[0])[1:-1])\n",
    "        except Exception as e:\n",
    "            generalized_text.append(word)\n",
    "\n",
    "    return ' '.join(generalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501b9d7",
   "metadata": {},
   "source": [
    "Przetwarzamy dane wejściowe tak, żeby były gotowe do uczenia modelu. Czyścimy, uogólniamy, kodujemy, przekształcamy tekst na wektory liczbowe (podobnie jak w pliku machine_learning.ipynb). Dzięki temu możemy potem podać dane do sieci neuronowej, która nauczy się rozpoznawać emocje i sentyment w języku polskim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591324bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mchoj\\Documents\\GitHub\\emotion-and-sentiment-recognition\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def delete_hashs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[~df['text'].astype(str).str.startswith('#')]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def encode_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear','Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']:\n",
    "        df[col] = df[col].apply(lambda x: 1 if x else 0)\n",
    "    return df\n",
    "\n",
    "def generalize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for index in df.index.to_list():\n",
    "        df.loc[index, 'text'] = generalize_text(str(df.loc[index, 'text']))\n",
    "    return df\n",
    "\n",
    "def embed_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    model = SentenceTransformer('sdadas/st-polish-paraphrase-from-distilroberta')\n",
    "    \n",
    "    corpus = [str(df.loc[index, 'text']) for index in df.index.to_list()]\n",
    "    embeddings = model.encode(corpus)\n",
    "\n",
    "    embedding_column_names = [f'embedding_{i}' for i in range(embeddings.shape[1])]\n",
    "    \n",
    "    df_embeddings = pd.DataFrame(embeddings, columns=embedding_column_names)\n",
    "    df = pd.concat([df_embeddings, df], axis=1)\n",
    "    return df\n",
    "\n",
    "def rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = df.columns.str.lower()\n",
    "    return df\n",
    "\n",
    "def transform_texts(df: pd.DataFrame) -> pd.DataFrame:    \n",
    "    result_df = delete_hashs(df=df)\n",
    "    result_df = encode_labels(df=result_df)\n",
    "    result_df = generalize(df=result_df)\n",
    "    result_df = embed_text(df=result_df)\n",
    "    result_df = rename_columns(df=result_df)\n",
    "    return result_df\n",
    "\n",
    "def delete_empty(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df['text'].astype(str).str.len() > 2]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def transform_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_df = pd.DataFrame(data={\n",
    "        'text': [],\n",
    "        'Joy': [], 'Trust': [], 'Anticipation': [], 'Surprise': [], 'Fear': [], 'Sadness': [],\n",
    "           'Disgust': [], 'Anger': [], 'Positive': [], 'Negative': [], 'Neutral': []\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for index in df.index.tolist():\n",
    "        if (str)(df.loc[index, 'text']).startswith('#'):\n",
    "            sentence = \" \".join(sentences)\n",
    "            df.loc[index, 'text'] = sentence\n",
    "            result_df = pd.concat([result_df, df.loc[[index]]])\n",
    "            sentences = []\n",
    "        else:\n",
    "            sentences.append((str)(df.loc[index, 'text']))\n",
    "            \n",
    "    result_df = delete_hashs(df=result_df)\n",
    "    result_df = delete_empty(df=result_df)\n",
    "    result_df = encode_labels(df=result_df)\n",
    "    result_df = generalize(df=result_df)\n",
    "    result_df = embed_text(df=result_df)\n",
    "    result_df = rename_columns(df=result_df)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3fe0a",
   "metadata": {},
   "source": [
    "Ładujemy dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a11bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "def load_data() -> List:\n",
    "    data = []\n",
    "    \n",
    "    for type in ['train', 'val', 'test']:\n",
    "        for category in ['texts', 'sentences']:\n",
    "            if os.path.exists(f'../data/clean/nn_{type}_{category}.csv'):\n",
    "                df = pd.read_csv(f'../data/clean/nn_{type}_{category}.csv', index_col=0)\n",
    "            else:\n",
    "                df = pd.read_csv(f'../data/raw/{type}.csv')\n",
    "                if category == 'texts':\n",
    "                    df = transform_texts(df=df)\n",
    "                elif category == 'sentences':\n",
    "                    df = transform_sentences(df=df)\n",
    "                df.to_csv(f'../data/clean/nn_{type}_{category}.csv')\n",
    "\n",
    "            data.append(df)        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bacae29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = data[0]\n",
    "train_sentences = data[1]\n",
    "val_texts = data[2]\n",
    "val_sentences = data[3]\n",
    "test_texts = data[4]\n",
    "test_sentences = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "060099ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def count_emotions(df: pd.DataFrame) -> Dict:\n",
    "    emotions = {\n",
    "        'Joy': 0, 'Trust': 0, 'Anticipation': 0, 'Surprise': 0, 'Fear': 0, 'Sadness': 0,\n",
    "        'Disgust': 0, 'Anger': 0, 'Positive': 0, 'Negative': 0, 'Neutral': 0\n",
    "    }\n",
    "\n",
    "    for emotion, i in zip(emotions.keys(), range(769, 780)):\n",
    "        emotions[emotion] = int(train_texts.iloc[:, i].sum())\n",
    "\n",
    "    return emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4da612",
   "metadata": {},
   "source": [
    "Tutaj liczymy, ile razy występuje każda emocja albo sentyment w zbiorze danych i tworzymy wykres słupkowy. Jest to jest ważne np. przy balansowaniu danych albo ocenianiu, czy model będzie miał trudniej z jakąś emocją.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de6d0e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = count_emotions(test_texts)\n",
    "\n",
    "plt.bar(counts.keys(), counts.values())\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edc1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3063)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.iloc[:, 769].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec526ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>trust</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>surprise</th>\n",
       "      <th>fear</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>anger</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.562764</td>\n",
       "      <td>-0.397882</td>\n",
       "      <td>-0.267292</td>\n",
       "      <td>-0.567739</td>\n",
       "      <td>-1.149851</td>\n",
       "      <td>0.662654</td>\n",
       "      <td>0.128688</td>\n",
       "      <td>0.475320</td>\n",
       "      <td>-0.341749</td>\n",
       "      <td>0.943553</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.025572</td>\n",
       "      <td>-0.235864</td>\n",
       "      <td>-0.290113</td>\n",
       "      <td>-0.991783</td>\n",
       "      <td>-0.482610</td>\n",
       "      <td>-0.079471</td>\n",
       "      <td>-0.227736</td>\n",
       "      <td>-1.217292</td>\n",
       "      <td>0.438888</td>\n",
       "      <td>-0.221104</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.475408</td>\n",
       "      <td>0.156776</td>\n",
       "      <td>-0.210054</td>\n",
       "      <td>-1.350016</td>\n",
       "      <td>-0.126225</td>\n",
       "      <td>-0.365511</td>\n",
       "      <td>0.108856</td>\n",
       "      <td>-0.143321</td>\n",
       "      <td>0.438478</td>\n",
       "      <td>0.077761</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.336756</td>\n",
       "      <td>0.084884</td>\n",
       "      <td>-0.059477</td>\n",
       "      <td>-0.480968</td>\n",
       "      <td>0.186055</td>\n",
       "      <td>-1.020058</td>\n",
       "      <td>-0.095881</td>\n",
       "      <td>0.175767</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.103957</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.477066</td>\n",
       "      <td>-0.384985</td>\n",
       "      <td>0.092087</td>\n",
       "      <td>-0.471120</td>\n",
       "      <td>-0.756108</td>\n",
       "      <td>-0.020734</td>\n",
       "      <td>-0.597373</td>\n",
       "      <td>-0.533785</td>\n",
       "      <td>0.817650</td>\n",
       "      <td>-0.484433</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.496382</td>\n",
       "      <td>-0.243892</td>\n",
       "      <td>-0.093434</td>\n",
       "      <td>-0.223938</td>\n",
       "      <td>0.163552</td>\n",
       "      <td>-0.107771</td>\n",
       "      <td>-0.155038</td>\n",
       "      <td>-0.587985</td>\n",
       "      <td>-0.201445</td>\n",
       "      <td>0.310824</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.118517</td>\n",
       "      <td>0.140505</td>\n",
       "      <td>-0.107383</td>\n",
       "      <td>-0.213710</td>\n",
       "      <td>-0.135078</td>\n",
       "      <td>-0.422372</td>\n",
       "      <td>0.039289</td>\n",
       "      <td>-0.559245</td>\n",
       "      <td>-0.158402</td>\n",
       "      <td>-0.107544</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.097196</td>\n",
       "      <td>-0.078515</td>\n",
       "      <td>-0.249001</td>\n",
       "      <td>-0.379476</td>\n",
       "      <td>-0.063541</td>\n",
       "      <td>-0.161061</td>\n",
       "      <td>-0.200802</td>\n",
       "      <td>-0.974016</td>\n",
       "      <td>-0.269598</td>\n",
       "      <td>-0.232394</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.251442</td>\n",
       "      <td>0.039844</td>\n",
       "      <td>-0.041393</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.021001</td>\n",
       "      <td>-0.140833</td>\n",
       "      <td>0.306735</td>\n",
       "      <td>0.057826</td>\n",
       "      <td>-0.162801</td>\n",
       "      <td>-0.397339</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.270282</td>\n",
       "      <td>0.079601</td>\n",
       "      <td>0.281395</td>\n",
       "      <td>-0.319867</td>\n",
       "      <td>0.225556</td>\n",
       "      <td>-0.520233</td>\n",
       "      <td>-0.212180</td>\n",
       "      <td>-0.398634</td>\n",
       "      <td>0.146555</td>\n",
       "      <td>-0.211571</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 780 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "0     0.562764    -0.397882    -0.267292    -0.567739    -1.149851   \n",
       "1    -0.025572    -0.235864    -0.290113    -0.991783    -0.482610   \n",
       "2    -0.475408     0.156776    -0.210054    -1.350016    -0.126225   \n",
       "3    -0.336756     0.084884    -0.059477    -0.480968     0.186055   \n",
       "4    -0.477066    -0.384985     0.092087    -0.471120    -0.756108   \n",
       "5    -0.496382    -0.243892    -0.093434    -0.223938     0.163552   \n",
       "6     0.118517     0.140505    -0.107383    -0.213710    -0.135078   \n",
       "7     0.097196    -0.078515    -0.249001    -0.379476    -0.063541   \n",
       "8     0.251442     0.039844    -0.041393     0.003911     0.021001   \n",
       "9     0.270282     0.079601     0.281395    -0.319867     0.225556   \n",
       "\n",
       "   embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "0     0.662654     0.128688     0.475320    -0.341749     0.943553  ...   \n",
       "1    -0.079471    -0.227736    -1.217292     0.438888    -0.221104  ...   \n",
       "2    -0.365511     0.108856    -0.143321     0.438478     0.077761  ...   \n",
       "3    -1.020058    -0.095881     0.175767    -0.000036     0.103957  ...   \n",
       "4    -0.020734    -0.597373    -0.533785     0.817650    -0.484433  ...   \n",
       "5    -0.107771    -0.155038    -0.587985    -0.201445     0.310824  ...   \n",
       "6    -0.422372     0.039289    -0.559245    -0.158402    -0.107544  ...   \n",
       "7    -0.161061    -0.200802    -0.974016    -0.269598    -0.232394  ...   \n",
       "8    -0.140833     0.306735     0.057826    -0.162801    -0.397339  ...   \n",
       "9    -0.520233    -0.212180    -0.398634     0.146555    -0.211571  ...   \n",
       "\n",
       "   trust  anticipation  surprise  fear  sadness  disgust  anger  positive  \\\n",
       "0      0             0         1     0        1        0      1         0   \n",
       "1      0             0         0     0        1        1      1         0   \n",
       "2      0             0         0     0        0        1      1         0   \n",
       "3      0             0         0     0        1        1      1         0   \n",
       "4      0             0         0     0        0        0      1         0   \n",
       "5      0             0         0     0        0        0      0         1   \n",
       "6      0             0         0     0        0        0      0         1   \n",
       "7      0             0         1     0        1        0      0         0   \n",
       "8      0             0         0     0        0        0      1         1   \n",
       "9      0             0         0     0        0        0      1         0   \n",
       "\n",
       "   negative  neutral  \n",
       "0         1        0  \n",
       "1         1        0  \n",
       "2         1        0  \n",
       "3         1        0  \n",
       "4         1        1  \n",
       "5         0        1  \n",
       "6         0        1  \n",
       "7         1        0  \n",
       "8         1        1  \n",
       "9         1        0  \n",
       "\n",
       "[10 rows x 780 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc06e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>trust</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>surprise</th>\n",
       "      <th>fear</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>anger</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.107992</td>\n",
       "      <td>-0.544149</td>\n",
       "      <td>0.053766</td>\n",
       "      <td>-0.632760</td>\n",
       "      <td>-0.311535</td>\n",
       "      <td>-0.581503</td>\n",
       "      <td>-0.313330</td>\n",
       "      <td>-0.348168</td>\n",
       "      <td>0.349289</td>\n",
       "      <td>-0.406877</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.311132</td>\n",
       "      <td>-0.028671</td>\n",
       "      <td>0.441330</td>\n",
       "      <td>0.269969</td>\n",
       "      <td>-0.503570</td>\n",
       "      <td>-0.387672</td>\n",
       "      <td>0.084179</td>\n",
       "      <td>0.129839</td>\n",
       "      <td>0.064349</td>\n",
       "      <td>-0.002668</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059933</td>\n",
       "      <td>-0.061721</td>\n",
       "      <td>0.278209</td>\n",
       "      <td>-0.076590</td>\n",
       "      <td>-0.328901</td>\n",
       "      <td>-0.568747</td>\n",
       "      <td>0.589950</td>\n",
       "      <td>-0.501475</td>\n",
       "      <td>0.192955</td>\n",
       "      <td>-0.235708</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.768429</td>\n",
       "      <td>0.329751</td>\n",
       "      <td>-0.046399</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>-0.539526</td>\n",
       "      <td>-0.464502</td>\n",
       "      <td>0.219784</td>\n",
       "      <td>-0.005870</td>\n",
       "      <td>0.543218</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.079323</td>\n",
       "      <td>-0.283436</td>\n",
       "      <td>-0.049123</td>\n",
       "      <td>0.551994</td>\n",
       "      <td>-0.150095</td>\n",
       "      <td>-0.236103</td>\n",
       "      <td>-0.445485</td>\n",
       "      <td>0.063526</td>\n",
       "      <td>-0.037673</td>\n",
       "      <td>0.159559</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008165</td>\n",
       "      <td>-0.044767</td>\n",
       "      <td>-0.019482</td>\n",
       "      <td>-0.543493</td>\n",
       "      <td>0.151010</td>\n",
       "      <td>-0.438446</td>\n",
       "      <td>-0.115624</td>\n",
       "      <td>-0.558962</td>\n",
       "      <td>-0.040636</td>\n",
       "      <td>-0.198771</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.192084</td>\n",
       "      <td>0.341302</td>\n",
       "      <td>0.191277</td>\n",
       "      <td>-0.035575</td>\n",
       "      <td>-0.619834</td>\n",
       "      <td>-0.517675</td>\n",
       "      <td>0.101253</td>\n",
       "      <td>-0.099043</td>\n",
       "      <td>-0.411317</td>\n",
       "      <td>-0.308393</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.034829</td>\n",
       "      <td>-0.051287</td>\n",
       "      <td>0.388698</td>\n",
       "      <td>0.251668</td>\n",
       "      <td>-0.303737</td>\n",
       "      <td>-0.325470</td>\n",
       "      <td>-0.421632</td>\n",
       "      <td>0.267487</td>\n",
       "      <td>-0.006021</td>\n",
       "      <td>-0.086745</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.085704</td>\n",
       "      <td>0.031369</td>\n",
       "      <td>0.434524</td>\n",
       "      <td>-0.142889</td>\n",
       "      <td>-0.652816</td>\n",
       "      <td>-0.535798</td>\n",
       "      <td>0.209822</td>\n",
       "      <td>-0.081895</td>\n",
       "      <td>-0.156835</td>\n",
       "      <td>-0.037204</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.134724</td>\n",
       "      <td>0.155525</td>\n",
       "      <td>0.143574</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>-0.135043</td>\n",
       "      <td>-0.729203</td>\n",
       "      <td>-0.419711</td>\n",
       "      <td>-0.199815</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.060348</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 780 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "0    -0.107992    -0.544149     0.053766    -0.632760    -0.311535   \n",
       "1     0.311132    -0.028671     0.441330     0.269969    -0.503570   \n",
       "2    -0.059933    -0.061721     0.278209    -0.076590    -0.328901   \n",
       "3    -0.768429     0.329751    -0.046399     0.045369    -0.539526   \n",
       "4    -0.079323    -0.283436    -0.049123     0.551994    -0.150095   \n",
       "5     0.008165    -0.044767    -0.019482    -0.543493     0.151010   \n",
       "6     0.192084     0.341302     0.191277    -0.035575    -0.619834   \n",
       "7    -0.034829    -0.051287     0.388698     0.251668    -0.303737   \n",
       "8    -0.085704     0.031369     0.434524    -0.142889    -0.652816   \n",
       "9     0.134724     0.155525     0.143574     0.011912    -0.135043   \n",
       "\n",
       "   embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "0    -0.581503    -0.313330    -0.348168     0.349289    -0.406877  ...   \n",
       "1    -0.387672     0.084179     0.129839     0.064349    -0.002668  ...   \n",
       "2    -0.568747     0.589950    -0.501475     0.192955    -0.235708  ...   \n",
       "3    -0.464502     0.219784    -0.005870     0.543218     0.019147  ...   \n",
       "4    -0.236103    -0.445485     0.063526    -0.037673     0.159559  ...   \n",
       "5    -0.438446    -0.115624    -0.558962    -0.040636    -0.198771  ...   \n",
       "6    -0.517675     0.101253    -0.099043    -0.411317    -0.308393  ...   \n",
       "7    -0.325470    -0.421632     0.267487    -0.006021    -0.086745  ...   \n",
       "8    -0.535798     0.209822    -0.081895    -0.156835    -0.037204  ...   \n",
       "9    -0.729203    -0.419711    -0.199815    -0.166667     0.060348  ...   \n",
       "\n",
       "   trust  anticipation  surprise  fear  sadness  disgust  anger  positive  \\\n",
       "0      0             0         1     0        1        1      1         0   \n",
       "1      0             0         0     0        0        0      0         0   \n",
       "2      0             0         0     0        0        0      0         1   \n",
       "3      0             0         0     0        1        0      0         1   \n",
       "4      1             0         0     0        0        0      0         1   \n",
       "5      0             0         0     0        1        0      1         0   \n",
       "6      0             1         0     0        0        0      0         1   \n",
       "7      0             0         0     0        1        0      0         0   \n",
       "8      1             1         0     0        0        0      0         1   \n",
       "9      0             0         0     0        0        0      0         1   \n",
       "\n",
       "   negative  neutral  \n",
       "0         1        0  \n",
       "1         0        1  \n",
       "2         0        1  \n",
       "3         1        0  \n",
       "4         0        0  \n",
       "5         1        0  \n",
       "6         0        0  \n",
       "7         1        0  \n",
       "8         0        1  \n",
       "9         0        0  \n",
       "\n",
       "[10 rows x 780 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a8e83",
   "metadata": {},
   "source": [
    "Przygotowanie danych wejściowych i walidacyjnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06400c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts = train_texts.iloc[:, :768] # + train_sentences.iloc[:, :768]\n",
    "y_train_texts = train_texts.iloc[:, 769:] # + train_sentences.iloc[:, 769:]\n",
    "\n",
    "X_val_texts = val_texts.iloc[:, :768] # + val_sentences.iloc[:, :768]\n",
    "y_val_texts = val_texts.iloc[:, 769:] # + val_sentences.iloc[:, 769:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1501f",
   "metadata": {},
   "source": [
    "Budowa i konfiguracja sieci neuronowej. Model to klasyczny Sequential w Kerasie, złożony z kilku gęstych warstw (Dense), warstw normalizacyjnych (BatchNormalization) i warstw z odrzuceniem (Dropout). Na końcu mamy warstwę z aktywacją sigmoid, bo przewidujemy wiele etykiet naraz (multilabel classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "92a7cb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mchoj\\Documents\\GitHub\\emotion-and-sentiment-recognition\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LayerNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "input_dim = X_train_texts.shape[1]\n",
    "output_dim = y_train_texts.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1024, activation='relu', input_shape=(input_dim, )),\n",
    "    BatchNormalization(),\n",
    "    # LayerNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    # LayerNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    # LayerNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    # LayerNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(output_dim, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4af616",
   "metadata": {},
   "source": [
    "Tutaj określamy sposób optymalizacji modelu sieci neuronowej oraz metryki uywane do oceny jakości predykcji.\n",
    "\n",
    "Optymalizator - algorytm Adam,który łączy zalety optymalizatorów AdaGrad oraz RMSProp +  mechanizm stopniowego wygaszania tempa;\n",
    "\n",
    "Podczas treningu monitorowane są trzy metryki:\n",
    "\n",
    "binary_accuracy – miara trafności klasyfikacji liczona oddzielnie dla każdej etykiety,\n",
    "\n",
    "precision – precyzja, czyli stosunek liczby trafnie przewidzianych pozytywnych przykładów do wszystkich przewidzianych jako pozytywne,\n",
    "\n",
    "recall – czułość, czyli stosunek trafnie wykrytych przykładów pozytywnych do wszystkich rzeczywistych pozytywnych przykładów.\n",
    "\n",
    "Wszystkie metryki są liczone przy zastosowaniu progu 0.5 – oznacza to, że jeśli model zwróci prawdopodobieństwo ≥ 0.5 dla danej etykiety, to zostanie ono zinterpretowane jako 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db176f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mchoj\\Documents\\GitHub\\emotion-and-sentiment-recognition\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=[\n",
    "        'binary_accuracy',\n",
    "        Precision(thresholds=0.5, name='precision'),\n",
    "        Recall(thresholds=0.5, name='recall'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796c9f5",
   "metadata": {},
   "source": [
    "Tu widać warstwa po warstwie, jak zbudowana jest nasza sieć. Zaczynamy od dużej warstwy z 1024 neuronami, potem stopniowo zmniejszamy liczbę neuronów aż do wyjścia z 11 predykcjami. Dodajemy BatchNormalization, żeby ustabilizować trening, i Dropout, żeby sieć się nie przeuczyła. Dzięki temu model jest dosyć głęboki, ale zabezpieczony przed zbyt dużym dopasowaniem do danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d390976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">787,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m787,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │         \u001b[38;5;34m1,419\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,830,539</span> (6.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,830,539\u001b[0m (6.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,826,699</span> (6.97 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,826,699\u001b[0m (6.97 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840</span> (15.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,840\u001b[0m (15.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5402e73",
   "metadata": {},
   "source": [
    "Tutaj trenujemy naszą sieć neuronową. Mamy dwie techniki wspomagające trening: EarlyStopping, który przerywa uczenie, jeśli model się nie poprawia, i ReduceLROnPlateau, który automatycznie zmniejsza learning rate, kiedy model utknie. Trenujemy maksymalnie 16 epok, ale jeśli model wcześniej osiągnie optimum na danych walidacyjnych, trening się wcześniej zakończy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cf3598f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - binary_accuracy: 0.7082 - loss: 0.5810 - precision: 0.4828 - recall: 0.5913 - val_binary_accuracy: 0.8475 - val_loss: 0.3604 - val_precision: 0.7896 - val_recall: 0.6193 - learning_rate: 0.0010\n",
      "Epoch 2/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - binary_accuracy: 0.8373 - loss: 0.3766 - precision: 0.7519 - recall: 0.5972 - val_binary_accuracy: 0.8584 - val_loss: 0.3399 - val_precision: 0.7990 - val_recall: 0.6591 - learning_rate: 0.0010\n",
      "Epoch 3/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - binary_accuracy: 0.8454 - loss: 0.3641 - precision: 0.7669 - recall: 0.6220 - val_binary_accuracy: 0.8598 - val_loss: 0.3353 - val_precision: 0.8008 - val_recall: 0.6633 - learning_rate: 0.0010\n",
      "Epoch 4/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - binary_accuracy: 0.8472 - loss: 0.3563 - precision: 0.7644 - recall: 0.6335 - val_binary_accuracy: 0.8580 - val_loss: 0.3367 - val_precision: 0.7881 - val_recall: 0.6728 - learning_rate: 0.0010\n",
      "Epoch 5/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - binary_accuracy: 0.8535 - loss: 0.3443 - precision: 0.7709 - recall: 0.6491 - val_binary_accuracy: 0.8575 - val_loss: 0.3341 - val_precision: 0.7836 - val_recall: 0.6770 - learning_rate: 0.0010\n",
      "Epoch 6/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - binary_accuracy: 0.8587 - loss: 0.3374 - precision: 0.7822 - recall: 0.6587 - val_binary_accuracy: 0.8571 - val_loss: 0.3312 - val_precision: 0.8015 - val_recall: 0.6494 - learning_rate: 0.0010\n",
      "Epoch 7/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - binary_accuracy: 0.8568 - loss: 0.3388 - precision: 0.7795 - recall: 0.6517 - val_binary_accuracy: 0.8552 - val_loss: 0.3387 - val_precision: 0.7924 - val_recall: 0.6530 - learning_rate: 0.0010\n",
      "Epoch 8/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - binary_accuracy: 0.8579 - loss: 0.3358 - precision: 0.7815 - recall: 0.6575 - val_binary_accuracy: 0.8631 - val_loss: 0.3330 - val_precision: 0.8144 - val_recall: 0.6607 - learning_rate: 0.0010\n",
      "Epoch 9/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - binary_accuracy: 0.8634 - loss: 0.3280 - precision: 0.7879 - recall: 0.6677 - val_binary_accuracy: 0.8588 - val_loss: 0.3323 - val_precision: 0.7809 - val_recall: 0.6878 - learning_rate: 0.0010\n",
      "Epoch 10/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - binary_accuracy: 0.8632 - loss: 0.3312 - precision: 0.7949 - recall: 0.6683 - val_binary_accuracy: 0.8658 - val_loss: 0.3208 - val_precision: 0.8207 - val_recall: 0.6649 - learning_rate: 0.0010\n",
      "Epoch 11/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - binary_accuracy: 0.8653 - loss: 0.3252 - precision: 0.7947 - recall: 0.6762 - val_binary_accuracy: 0.8608 - val_loss: 0.3348 - val_precision: 0.8164 - val_recall: 0.6472 - learning_rate: 0.0010\n",
      "Epoch 12/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - binary_accuracy: 0.8734 - loss: 0.3071 - precision: 0.8088 - recall: 0.6893 - val_binary_accuracy: 0.8581 - val_loss: 0.3332 - val_precision: 0.7879 - val_recall: 0.6736 - learning_rate: 0.0010\n",
      "Epoch 13/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - binary_accuracy: 0.8703 - loss: 0.3130 - precision: 0.8010 - recall: 0.6888 - val_binary_accuracy: 0.8629 - val_loss: 0.3272 - val_precision: 0.7851 - val_recall: 0.7013 - learning_rate: 0.0010\n",
      "Epoch 14/16\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - binary_accuracy: 0.8714 - loss: 0.3116 - precision: 0.8025 - recall: 0.6955 - val_binary_accuracy: 0.8650 - val_loss: 0.3218 - val_precision: 0.8023 - val_recall: 0.6860 - learning_rate: 0.0010\n",
      "Epoch 15/16\n",
      "\u001b[1m797/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.8731 - loss: 0.3041 - precision: 0.8021 - recall: 0.7017\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - binary_accuracy: 0.8731 - loss: 0.3041 - precision: 0.8021 - recall: 0.7016 - val_binary_accuracy: 0.8675 - val_loss: 0.3228 - val_precision: 0.8027 - val_recall: 0.6973 - learning_rate: 0.0010\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_texts, y_train_texts,\n",
    "    epochs=16,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_val_texts, y_val_texts),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dba875d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/neural_network.keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f149ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "history = keras.models.load_model('../models/neural_network.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb862e",
   "metadata": {},
   "source": [
    "Tutaj ręcznie liczymy klasyczne metryki klasyfikacji – precision, recall i F1 – dla każdej emocji osobno.  Obliczamy metryki indywidualne oraz średnią z wyników dla wszystkich etykiet. Dzięki temu możemy ocenić, jak dobrze model radzi sobie z każdą emocją z osobna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d5ee194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "def evaluate_texts(model, X_test: pd.DataFrame, y_test: pd.DataFrame, threshold: float=0.5) -> Dict:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    y_test = y_test.values\n",
    "    \n",
    "    labels = ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear', 'Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['F1-score macro texts'] = 0\n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = 0\n",
    "        metrics[f\"Recall {label}\"] = 0\n",
    "        metrics[f\"F1-score {label}\"] = 0\n",
    "        metrics[f\"TP {label}\"] = 0\n",
    "        metrics[f\"FP {label}\"] = 0\n",
    "        metrics[f\"TN {label}\"] = 0\n",
    "        metrics[f\"FN {label}\"] = 0\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"TP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 1)))\n",
    "        metrics[f\"FP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"TN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"FN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 1)))\n",
    "        \n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FP {label}\"] + 1e-8)\n",
    "        metrics[f\"Recall {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FN {label}\"] + 1e-8)\n",
    "        metrics[f\"F1-score {label}\"] = 2 * (metrics[f\"Precision {label}\"] * metrics[f\"Recall {label}\"]) / (metrics[f\"Precision {label}\"] + metrics[f\"Recall {label}\"] + 1e-8)\n",
    "\n",
    "    metrics['F1-score macro texts'] = sum(metrics[f\"F1-score {label}\"] for label in labels) / len(labels)\n",
    "    \n",
    "    metrics = {k: v for (k, v) in metrics.items() if ('Precision' in k) or ('Recall' in k) or ('F1' in k)}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d785fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentences_v1(model, X_test: pd.DataFrame, y_test: pd.DataFrame, threshold: float=0.5) -> Dict:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    y_test = y_test.values\n",
    "    \n",
    "    labels = ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear', 'Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['F1-score macro sentences'] = 0\n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = 0\n",
    "        metrics[f\"Recall {label}\"] = 0\n",
    "        metrics[f\"F1-score {label}\"] = 0\n",
    "        metrics[f\"TP {label}\"] = 0\n",
    "        metrics[f\"FP {label}\"] = 0\n",
    "        metrics[f\"TN {label}\"] = 0\n",
    "        metrics[f\"FN {label}\"] = 0\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"TP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 1)))\n",
    "        metrics[f\"FP {label}\"] = int(np.sum((y_pred_binary[:, i] == 1) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"TN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 0)))\n",
    "        metrics[f\"FN {label}\"] = int(np.sum((y_pred_binary[:, i] == 0) & (y_test[:, i] == 1)))\n",
    "        \n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FP {label}\"] + 1e-8)\n",
    "        metrics[f\"Recall {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FN {label}\"] + 1e-8)\n",
    "        metrics[f\"F1-score {label}\"] = 2 * (metrics[f\"Precision {label}\"] * metrics[f\"Recall {label}\"]) / (metrics[f\"Precision {label}\"] + metrics[f\"Recall {label}\"] + 1e-8)\n",
    "\n",
    "    metrics['F1-score macro sentences'] = sum(metrics[f\"F1-score {label}\"] for label in labels) / len(labels)\n",
    "\n",
    "    metrics = {k: v for (k, v) in metrics.items() if ('Precision' in k) or ('Recall' in k) or ('F1' in k)}\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30adb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_indeces(df: pd.DataFrame) -> List:\n",
    "    hash_indices = []\n",
    "    for index in df.index.tolist():\n",
    "        if str(df.loc[index, 'text']).startswith('#'):\n",
    "            hash_indices.append(index)\n",
    "          \n",
    "    result_hash_indices = []   \n",
    "    i = 0\n",
    "    for index in hash_indices:\n",
    "        index = index - i\n",
    "        result_hash_indices.append(index)\n",
    "        i += 1\n",
    "    \n",
    "    return result_hash_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61f8f6",
   "metadata": {},
   "source": [
    "Ta funkcja ocenia model na poziomie całych recenzji. Dla każdej recenzji sumujemy przewidywane prawdopodobieństwa dla wszystkich jej zdań i sprawdzamy, które emocje były  przekroczyły zadany próg. Potem porównujemy te przewidywania z rzeczywistymi etykietami i liczymy metryki precision, recall i F1 osobno dla każdej emocji. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e440e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentences_v2(model: keras.Model, X_test: pd.DataFrame, y_test: pd.DataFrame, threshold: float=1): # -> Dict:\n",
    "    raw_data = encode_labels(pd.read_csv('../data/raw/test.csv'))\n",
    "    hash_indices = get_hash_indeces(raw_data)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = y_test.values\n",
    "    \n",
    "    labels = ['Joy', 'Trust', 'Anticipation', 'Surprise', 'Fear', 'Sadness', 'Disgust', 'Anger', 'Positive', 'Negative', 'Neutral']\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics['F1-score macro sentences'] = 0\n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = 0\n",
    "        metrics[f\"Recall {label}\"] = 0\n",
    "        metrics[f\"F1-score {label}\"] = 0\n",
    "        metrics[f\"TP {label}\"] = 0\n",
    "        metrics[f\"FP {label}\"] = 0\n",
    "        metrics[f\"TN {label}\"] = 0\n",
    "        metrics[f\"FN {label}\"] = 0\n",
    "    \n",
    "    y_true_segments = []\n",
    "    y_pred_segments = []\n",
    "    \n",
    "    i = 0\n",
    "    start = 0\n",
    "    for index in hash_indices:\n",
    "        pred_sum = np.zeros(11, dtype=np.float64)\n",
    "        for y_pred_i in y_pred[start:index]:\n",
    "            pred_sum += y_pred_i\n",
    "            \n",
    "        y_true_segments.append(raw_data.iloc[index+0, 1:].to_numpy())\n",
    "        y_pred_segments.append((pred_sum >= threshold).astype(int))\n",
    "                    \n",
    "        i += 1\n",
    "        start = index\n",
    "\n",
    "    y_true_segments = np.array(y_true_segments)\n",
    "    y_pred_segments = np.array(y_pred_segments)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        metrics[f\"TP {label}\"] = int(np.sum((y_pred_segments[:, i] == 1) & (y_true_segments[:, i] == 1)))\n",
    "        metrics[f\"FP {label}\"] = int(np.sum((y_pred_segments[:, i] == 1) & (y_true_segments[:, i] == 0)))\n",
    "        metrics[f\"TN {label}\"] = int(np.sum((y_pred_segments[:, i] == 0) & (y_true_segments[:, i] == 0)))\n",
    "        metrics[f\"FN {label}\"] = int(np.sum((y_pred_segments[:, i] == 0) & (y_true_segments[:, i] == 1)))\n",
    "        \n",
    "    for label in labels:\n",
    "        metrics[f\"Precision {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FP {label}\"] + 1e-8)\n",
    "        metrics[f\"Recall {label}\"] = metrics[f\"TP {label}\"] / (metrics[f\"TP {label}\"] + metrics[f\"FN {label}\"] + 1e-8)\n",
    "        metrics[f\"F1-score {label}\"] = 2 * (metrics[f\"Precision {label}\"] * metrics[f\"Recall {label}\"]) / (metrics[f\"Precision {label}\"] + metrics[f\"Recall {label}\"] + 1e-8)\n",
    "\n",
    "    metrics['F1-score macro sentences'] = sum(metrics[f\"F1-score {label}\"] for label in labels) / len(labels)\n",
    "\n",
    "    metrics = {k: v for (k, v) in metrics.items() if ('Precision' in k) or ('Recall' in k) or ('F1' in k)}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36c353e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "{'F1-score macro texts': 0.5589679193423925, 'Precision Joy': 0.7999999999857142, 'Recall Joy': 0.8145454545306446, 'F1-score Joy': 0.8072072021930689, 'Precision Trust': 0.6936416184570149, 'Recall Trust': 0.5741626793983654, 'F1-score Trust': 0.6282722463204135, 'Precision Anticipation': 0.6458333331987847, 'Recall Anticipation': 0.24799999998016, 'F1-score Anticipation': 0.3583814988392529, 'Precision Surprise': 0.0, 'Recall Surprise': 0.0, 'F1-score Surprise': 0.0, 'Precision Fear': 0.6923076917751478, 'Recall Fear': 0.1525423728555013, 'F1-score Fear': 0.24999999697145062, 'Precision Sadness': 0.868978805378247, 'Recall Sadness': 0.8199999999850909, 'F1-score Sadness': 0.8437792279163885, 'Precision Disgust': 0.69599999994432, 'Recall Disgust': 0.3580246913432912, 'F1-score Disgust': 0.4728260824449138, 'Precision Anger': 0.8169014083356477, 'Recall Anger': 0.27619047617732423, 'F1-score Anger': 0.41281138409442636, 'Precision Positive': 0.7787878787760789, 'Recall Positive': 0.8237179487047481, 'F1-score Positive': 0.8006230479509613, 'Precision Negative': 0.8693284936321356, 'Recall Negative': 0.8287197231690533, 'F1-score Negative': 0.8485385246601043, 'Precision Neutral': 0.8464163822236718, 'Recall Neutral': 0.6358974358811308, 'F1-score Neutral': 0.7262079013753379}\n"
     ]
    }
   ],
   "source": [
    "texts_metrics = evaluate_texts(history, X_test=test_texts.iloc[:, :768], y_test=test_texts.iloc[:, 769:], threshold=0.5)\n",
    "print(texts_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a702b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "{'F1-score macro sentences': 0.5880950400409627, 'Precision Joy': 0.7894736841274239, 'Recall Joy': 0.8720930231544078, 'F1-score Joy': 0.8287292766887457, 'Precision Trust': 0.695652173610586, 'Recall Trust': 0.5333333331555555, 'F1-score Trust': 0.603773579765041, 'Precision Anticipation': 0.9999999900000002, 'Recall Anticipation': 0.08333333326388888, 'F1-score Anticipation': 0.1538461521893491, 'Precision Surprise': 0.0, 'Recall Surprise': 0.0, 'F1-score Surprise': 0.0, 'Precision Fear': 0.571428570612245, 'Recall Fear': 0.3333333330555555, 'F1-score Fear': 0.42105262648199443, 'Precision Sadness': 0.8723404254391128, 'Recall Sadness': 0.9010989009998793, 'F1-score Sadness': 0.8864864813919651, 'Precision Disgust': 0.6285714283918368, 'Recall Disgust': 0.4888888887802469, 'F1-score Disgust': 0.5499999949406251, 'Precision Anger': 0.839999999664, 'Recall Anger': 0.46666666656296296, 'F1-score Anger': 0.5999999952367348, 'Precision Positive': 0.807692307614645, 'Recall Positive': 0.8659793813540227, 'F1-score Positive': 0.8358208904452861, 'Precision Negative': 0.8799999999120001, 'Recall Negative': 0.9462365590380392, 'F1-score Negative': 0.9119170933576741, 'Precision Neutral': 0.9130434778638941, 'Recall Neutral': 0.5384615383234714, 'F1-score Neutral': 0.6774193499531739}\n"
     ]
    }
   ],
   "source": [
    "sentences_metrics_v1 = evaluate_sentences_v1(history, X_test=test_sentences.iloc[:, :768], y_test=test_sentences.iloc[:, 769:], threshold=0.5)\n",
    "print(sentences_metrics_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78d16f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "{'F1-score macro sentences': 0.15919378981692603, 'Precision Joy': 0.4999999998076923, 'Recall Joy': 0.18309859152350727, 'F1-score Joy': 0.2680412331342332, 'Precision Trust': 0.21739130425330813, 'Recall Trust': 0.19999999992, 'F1-score Trust': 0.20833332825520845, 'Precision Anticipation': 0.04761904759637188, 'Recall Anticipation': 0.0624999999609375, 'F1-score Anticipation': 0.054054049116143615, 'Precision Surprise': 0.0999999999, 'Recall Surprise': 0.0999999999, 'F1-score Surprise': 0.09999999490000026, 'Precision Fear': 0.0, 'Recall Fear': 0.0, 'F1-score Fear': 0.0, 'Precision Sadness': 0.5185185183264746, 'Recall Sadness': 0.1794871794641683, 'F1-score Sadness': 0.26666666279546486, 'Precision Disgust': 0.12499999994791666, 'Recall Disgust': 0.09999999996666667, 'F1-score Disgust': 0.11111110613168747, 'Precision Anger': 0.14285714278911565, 'Recall Anger': 0.0909090908815427, 'F1-score Anger': 0.11111110631687264, 'Precision Positive': 0.5384615382544379, 'Recall Positive': 0.18181818179456907, 'F1-score Positive': 0.27184465636723537, 'Precision Negative': 0.5357142855229592, 'Recall Negative': 0.18749999997656253, 'F1-score Negative': 0.2777777738854596, 'Precision Neutral': 0.16666666657407406, 'Recall Neutral': 0.05454545453553719, 'F1-score Neutral': 0.08219177708388081}\n"
     ]
    }
   ],
   "source": [
    "sentences_metrics_v2 = evaluate_sentences_v2(history, X_test=test_sentences.iloc[:, :768], y_test=test_sentences.iloc[:, 769:], threshold=0.5)\n",
    "print(sentences_metrics_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa686b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_score(text_metrics: Dict, sentences_metrics: Dict) -> float:\n",
    "    return (text_metrics['F1-score macro texts'] + sentences_metrics['F1-score macro sentences']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "677e8105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5735314796916776"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_final_score(texts_metrics, sentences_metrics_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80c70a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35908085457965927"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_final_score(texts_metrics, sentences_metrics_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74cda7",
   "metadata": {},
   "source": [
    "Model najlepiej radzi sobie, gdy analizujemy emocje na poziomie pojedynczych zdań (wersja v1) lub całych recenzji.\n",
    "Próba agregowania predykcji zdań z powrotem do poziomu recenzji (v2) wypada słabo, najpewniej dlatego, że sumowanie predykcji i próg 1 są zbyt wymagające.\n",
    "Finalny F1 w najlepszym wariancie wynosi ~0.57, co jest przyzwoitym wynikiem w klasyfikacji wieloetykietowej na danych języka polskiego."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
